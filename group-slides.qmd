---
title: "Forest Foresight Slides"
subtitle: "Lit Review Slides"
author: "Forest Foresight (Advisor: Dr. Seals)"
date: last-modified
date-format: long
format:
  revealjs:
    theme: dark
    css: custom.css
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
csl: ieee.csl
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Random Forest in Machine Learning: A Review of Recent Advancements

## Abstract
This paper reviews six influential works in the field of machine learning, with a focus on Random Forest algorithms and their applications. We examine the development of interpretation methods, the foundational concepts of statistical learning, the theoretical underpinnings of Random Forests, and recent empirical studies comparing various classifiers. The review highlights the versatility and effectiveness of Random Forests while also addressing challenges such as bias in variable importance measures.

 
## 1. Introduction
Machine learning techniques, particularly Random Forest algorithms, have become increasingly important in various domains due to their robustness and versatility. This paper synthesizes findings from six key papers that have contributed significantly to our understanding and application of Random Forests and related techniques in machine learning.


## 2. Methodology
This review examines six papers published between 2001 and 2017, covering theoretical foundations, practical applications, and comparative studies of Random Forests and related machine learning techniques.

## 3. Results and Discussion

## 3.1 Unified Approach to Model Interpretation {.smaller}
(Lundberg and Lee 2017) address the crucial challenge of balancing model complexity with interpretability. They introduce SHAP (SHapley Additive exPlanations), a unified framework for interpreting model predictions. SHAP, grounded in game theory, provides a fair method to distribute the contribution of each feature to the final prediction. This approach ensures that the interpretation method satisfies important properties such as local accuracy, missingness, and consistency. SHAP's ability to encompass various existing interpretation methods allows for a more standardized approach to model interpretation across different types of models and domains. [@Lundberg2017-ww]

## 3.2 Foundations of Statistical Learning {.smaller}
(Hastie et al. 2009) provide a comprehensive overview of statistical learning techniques, with a particular focus on tree-based methods. Their work explores decision trees in depth, highlighting their versatility in both classification and regression tasks. The authors also discuss powerful ensemble methods such as bagging, boosting, and random forests, explaining how these techniques combine multiple models to improve predictive performance. This foundational text provides readers with a solid understanding of the advantages and potential drawbacks of these methods. [@Hastie2009-ck]


## 3.3 Theoretical and Practical Aspects of Random Forests {.smaller}
(Louppe 2014) offers a comprehensive examination of Random Forests, covering both theoretical foundations and practical applications. The paper explains the tree construction process in Random Forests, emphasizing the importance of random feature subset selection in building robust models. Louppe also discusses feature importance in Random Forests, explaining how to determine the importance of various features by measuring the decrease in impurity across all trees in the forest. The concept of out-of-bag error is explored as a unique advantage of Random Forests for internal validation.[@Louppe2014-pi]

## 3.4 Introduction and Theoretical Insights into Random Forests {.smaller}
Breiman's seminal paper (Breiman 2001) introduces the Random Forest algorithm, which has since become one of the most popular and effective machine learning techniques. Breiman provides a thorough explanation of the algorithm's foundations, demonstrating how it combines bagging with random feature selection at each node of the decision trees. The paper offers theoretical insights into why Random Forests work so well, discussing the impact of random feature selection on tree correlation and overall forest error rate. Breiman also introduces the concept of "tree strength" and its relationship to forest performance.[@Breiman2001-il]

## 3.5 Comparative Study of Classifiers {.smaller}
(Fernández-Delgado et al. 2014) present a comprehensive empirical study comparing 179 classifiers from 17 different families across 121 datasets. Their findings strongly support the effectiveness of Random Forests, which consistently rank among the top-performing classifiers across a wide range of problems. The study discusses trade-offs between different classifier families, considering factors such as accuracy, computational efficiency, and ease of use. This extensive comparison offers valuable insights into when Random Forests might be particularly effective and provides guidance for practitioners in choosing appropriate algorithms for specific problems.[@Fernandez-Delgado2014-iz]

## 3.6 Addressing Bias in Random Forest Variable Importance Measures {.smaller}
(Strobl et al. 2007) address a critical issue in the interpretation of Random Forests: bias in variable importance measures. The authors demonstrate that standard variable importance measures in Random Forests can be biased, particularly when predictor variables vary in their scale of measurement or number of categories. To address this issue, they propose a conditional permutation importance measure that accounts for correlations between predictor variables, providing a more accurate assessment of each variable's true importance. This work highlights the need for careful consideration of potential biases in machine learning interpretation techniques.[@Strobl2007-ks]

## 4. Conclusion {.smaller}
This review of six influential papers in the field of machine learning, with a focus on Random Forests, highlights the significant advancements made in both theoretical understanding and practical applications of these techniques. From the development of unified interpretation frameworks to comprehensive empirical studies and the addressing of biases in importance measures, these works collectively demonstrate the power and versatility of Random Forests while also acknowledging the challenges that remain. Future research should continue to build on these foundations, addressing remaining limitations and expanding the application of Random Forests to new domains.

## Lit Review 1 {.smaller .scrollable}

The book *Data Mining with Decision Trees: Theory and Applications* [@Horobet2023-bv] provides an in-depth look at decision trees, a popular tool in data mining for their simplicity, transparency, and efficiency. Decision trees are easy to follow, require minimal computational effort, and perform well on large datasets with various data types. The book explains how to prevent overfitting using pre-pruning, post-pruning, and cost-complexity pruning, and covers evaluation techniques like cross-validation and bootstrapping. It also discusses common algorithms (ID3, C4.5, CART) and the limitations of decision trees, such as their tendency to overlook attribute combinations. Overall, it’s a valuable guide for understanding decision trees and their applications.

## Lit Review 2 {.smaller .scrollable}

The paper "Financial Forecast in Business and an Application Proposal: The Case of Random Forest Technique" [@Orhan2023-hm] examines the effectiveness of the Random Forest (RF) algorithm in predicting financial performance. Using data from five companies listed on Borsa Istanbul (2009-2020), key financial variables were estimated with the RF technique. The model utilized 113 variables, including macroeconomic indicators like inflation and GDP growth, achieving a 90.9% accuracy rate. The study concludes that RF is a reliable tool for financial forecasting, especially when incorporating macroeconomic factors, but suggests further improvements by adding more external variables.

## Lit Review 3 {.smaller .scrollable}

Leo Breiman’s 2001 paper on Random Forests [@Breiman_undated-tp] introduces the algorithm as a classifier made up of multiple decision trees, each voting for the most popular class. It emphasizes Random Forests' ability to improve classification accuracy through tree diversity and explains that accuracy depends on the strength and independence of individual trees. Breiman highlights the advantages of Random Forests, including handling large, high-dimensional datasets, estimating variable importance, and managing missing data. The method works well for classification, less so for regression, and shows lower error rates on larger datasets. Different types of randomness can also enhance results.

## Lit Review 4 {.smaller .scrollable}

The paper "Random Forests for Classification in Ecology" [@Cutler2007-pe] explores the underutilized Random Forest (RF) method in ecological studies. RF works by creating multiple classification trees from bootstrap samples and combining their predictions, improving accuracy compared to single trees. It excels with ecological data, which is often high-dimensional, complex, and has many missing values. In the study, RF was tested on three ecological datasets—plant invasions, rare lichen, and cavity-nesting birds—and outperformed other methods (LDA, logistic regression, etc.), showing high accuracy. The paper advocates for broader adoption of RF in ecology.

## Lit Review 5 {.smaller .scrollable}

J.R. Quinlan's paper [@Quinlan1987-cz] addresses the complexity of decision trees in expert systems, proposing four methods for simplifying them: Cost-Complexity Pruning, Reduced Error Pruning, Pessimistic Pruning, and Simplifying to Production Rules. These methods reduce tree complexity by pruning irrelevant conditions. Tested across six domains (e.g., Hypothyroid diagnosis, Chess Endgame), all methods significantly reduced tree size, improving interpretability. For example, in the Hypothyroid domain, tree nodes were reduced from 23.6 to as low as 11.0, while the Production Rules method simplified the tree to just 3 rules. The study shows that simplifying decision trees improves clarity and utility in expert systems.

## Lit Review 6 {.smaller .scrollable}

Bain and Mason's framework on business performance, later refined by Porter (1979), Schmalensee (1985), and Rumelt (1991), serves as the basis for the study titled "Foreign Versus Local Ownership and Performance in Eastern Versus Western EU: A Random Forest Application." [@Horobet2023-bv] This paper uses Random Forest to examine the effects of foreign and local ownership on business performance in Eastern and Western EU firms between 2009 and 2016, covering 27 industries across 9 sectors. Key variables include personnel costs, labor productivity, and gross investment. The results show that personnel costs per employee are the most significant factor distinguishing foreign from local ownership. Locally owned companies outperformed foreign-owned ones in eight out of nine sectors, particularly in gross investments relative to turnover and value added to turnover ratios, indicating a stronger investment propensity.

## Literature Review {.smaller}

-   RF outperforms models like Naive Bayes and SVM by adjusting tree numbers and feature selection, using AUC as the main performance metric.[@Khoshgoftaar2007-bo]

-   RF handles both classification and regression tasks by creating multiple decision trees, offering resilience against overfitting.[@Biau2016-ac]

-   RF outperforms logistic regression in predicting retention and profitability, emphasizing the role of sales behavior in customer outcomes.[@Lariviere2005-uq]

-   RF accurately identifies core suppliers using e-invoice data with an AUC of 89.58%, improving procurement accuracy while reducing human bias.[@Hong2018-fr]

-   RF, using Non-Negative Matrix Factorization (NMF), classifies customer reviews with 81% precision, offering valuable insights for tailoring business offerings to specific customer needs.[@Torizuka2018-if]

-   Random Credal RF uses credal sets for increased robustness and accuracy, handling noisy datasets better than Oblique RF, but with added complexity.[@Mantas2019-li]

## References

::: {#refs .small}
:::