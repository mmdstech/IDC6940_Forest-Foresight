---
title: "Forest Foresight Slides"
subtitle: "Lit Review Slides"
author: "Forest Foresight (Advisor: Dr. Seals)"
date: last-modified
date-format: long
format:
  revealjs:
    theme: dark
    css: custom.css
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
csl: ieee.csl
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Random Forest in Machine Learning: A Review of Recent Advancements

## Abstract
This paper reviews six influential works in the field of machine learning, with a focus on Random Forest algorithms and their applications. We examine the development of interpretation methods, the foundational concepts of statistical learning, the theoretical underpinnings of Random Forests, and recent empirical studies comparing various classifiers. The review highlights the versatility and effectiveness of Random Forests while also addressing challenges such as bias in variable importance measures.

 
## 1. Introduction
Machine learning techniques, particularly Random Forest algorithms, have become increasingly important in various domains due to their robustness and versatility. This paper synthesizes findings from six key papers that have contributed significantly to our understanding and application of Random Forests and related techniques in machine learning.


## 2. Methodology
This review examines six papers published between 2001 and 2017, covering theoretical foundations, practical applications, and comparative studies of Random Forests and related machine learning techniques.

## 3. Results and Discussion

## 3.1 Unified Approach to Model Interpretation {.smaller}
(Lundberg and Lee 2017) address the crucial challenge of balancing model complexity with interpretability. They introduce SHAP (SHapley Additive exPlanations), a unified framework for interpreting model predictions. SHAP, grounded in game theory, provides a fair method to distribute the contribution of each feature to the final prediction. This approach ensures that the interpretation method satisfies important properties such as local accuracy, missingness, and consistency. SHAP's ability to encompass various existing interpretation methods allows for a more standardized approach to model interpretation across different types of models and domains. [@Lundberg2017-ww]

## 3.2 Foundations of Statistical Learning {.smaller}
(Hastie et al. 2009) provide a comprehensive overview of statistical learning techniques, with a particular focus on tree-based methods. Their work explores decision trees in depth, highlighting their versatility in both classification and regression tasks. The authors also discuss powerful ensemble methods such as bagging, boosting, and random forests, explaining how these techniques combine multiple models to improve predictive performance. This foundational text provides readers with a solid understanding of the advantages and potential drawbacks of these methods. [@Hastie2009-ck]


## 3.3 Theoretical and Practical Aspects of Random Forests {.smaller}
(Louppe 2014) offers a comprehensive examination of Random Forests, covering both theoretical foundations and practical applications. The paper explains the tree construction process in Random Forests, emphasizing the importance of random feature subset selection in building robust models. Louppe also discusses feature importance in Random Forests, explaining how to determine the importance of various features by measuring the decrease in impurity across all trees in the forest. The concept of out-of-bag error is explored as a unique advantage of Random Forests for internal validation.[@Louppe2014-pi]

## 3.4 Introduction and Theoretical Insights into Random Forests {.smaller}
Breiman's seminal paper (Breiman 2001) introduces the Random Forest algorithm, which has since become one of the most popular and effective machine learning techniques. Breiman provides a thorough explanation of the algorithm's foundations, demonstrating how it combines bagging with random feature selection at each node of the decision trees. The paper offers theoretical insights into why Random Forests work so well, discussing the impact of random feature selection on tree correlation and overall forest error rate. Breiman also introduces the concept of "tree strength" and its relationship to forest performance.[@Breiman2001-il]

## 3.5 Comparative Study of Classifiers {.smaller}
(Fernández-Delgado et al. 2014) present a comprehensive empirical study comparing 179 classifiers from 17 different families across 121 datasets. Their findings strongly support the effectiveness of Random Forests, which consistently rank among the top-performing classifiers across a wide range of problems. The study discusses trade-offs between different classifier families, considering factors such as accuracy, computational efficiency, and ease of use. This extensive comparison offers valuable insights into when Random Forests might be particularly effective and provides guidance for practitioners in choosing appropriate algorithms for specific problems.[@Fernandez-Delgado2014-iz]

## 3.6 Addressing Bias in Random Forest Variable Importance Measures {.smaller}
(Strobl et al. 2007) address a critical issue in the interpretation of Random Forests: bias in variable importance measures. The authors demonstrate that standard variable importance measures in Random Forests can be biased, particularly when predictor variables vary in their scale of measurement or number of categories. To address this issue, they propose a conditional permutation importance measure that accounts for correlations between predictor variables, providing a more accurate assessment of each variable's true importance. This work highlights the need for careful consideration of potential biases in machine learning interpretation techniques.[@Strobl2007-ks]

## 4. Conclusion {.smaller}
This review of six influential papers in the field of machine learning, with a focus on Random Forests, highlights the significant advancements made in both theoretical understanding and practical applications of these techniques. From the development of unified interpretation frameworks to comprehensive empirical studies and the addressing of biases in importance measures, these works collectively demonstrate the power and versatility of Random Forests while also acknowledging the challenges that remain. Future research should continue to build on these foundations, addressing remaining limitations and expanding the application of Random Forests to new domains.

## Lit Review 1 {.smaller .scrollable}

Bain and Mason (Porter, 1981) originally developed “The theoretical framework examining business performance” [@Horobet2023-bv] during 1940-1950 with further refinements by Porter (1979), Schmalensee (1985) and Rumelt (1991).  The paper titled "Foreign Versus Local Ownership and Performance in Eastern Versus Western EU: A Random Forest Application" uses Random Forest methodology to explore foreign and local ownership effects on business performance in the Eastern and Western firms of the European Union between 2009 and 2016. It included 27 industries from 9 sectors. Key variables include personnel costs, labor productivity, and gross investment. The model results show that personnel costs per employee are the most significant variable differentiating foreign from locally owned companies. The model was applied to 1,080 business units from the EU, each from a different sector and industry, but also region (Eastern or Western part of the EU), and operating under different ownership, i.e., foreign or locally-owned. The main objective of classifying economic activity within the EU is that the authors wanted to identify whether business performance brought on by foreign versus local ownership may be explained by headquarters’ location, industry of operation and a reduced set of other performance variables. It was concluded that locally owned companies have an edge against the foreign-owned ones in terms of the importance of their gross investments compared to turnover and of the ratio between value added and turnover in eight out of nine sectors. These results are a sign that there is a stronger propensity of locally-owned companies towards investments versus a weaker investment activity of foreign-owned companies.

## Lit Review 2 {.smaller .scrollable}

The paper "Financial Forecast in Business and an Application Proposal: The Case of Random Forest Technique" [@Orhan2023-hm] explores the viability of the Random Forest (RF) algorithm in predicting future financial performance. The study uses data from five companies whose shares have been traded on Borsa Istanbul between 2009 and 2020. Financial statements (between 2009 and 2020) of these businesses were obtained from the Public Disclosure Platform website. Variables such as current & fixed assets, equity, revenue, and net income have been estimated by use of the random forest technique.  As stated in the paper, “Random Forest is frequently preferred in classification and regression analyses because it produces reliable results by using the average of more than one decision tree and allows working with any number of trees (Biau-Scornet,2016:198).”  By leveraging 113 variables, including macroeconomic indicators like inflation, exchange rates, and GDP growth, the RF model showed an overall forecasting accuracy of 90.9%.  The research concludes that Random Forest is an effective tool for financial forecasting in businesses, particularly when coupled with non-financial and macroeconomic factors. However, further enhancements, such as the inclusion of additional external variables, could improve its predictive power in volatile periods like 2020. The study promotes RF as a valuable model for decision-makers in financial planning and risk management, offering high reliability in predicting future financial outcomes.

## Lit Review 3 {.smaller .scrollable}

As stated in Leo Breiman’s paper on Random Forests, published in January of 2001: “A random forest is a classifier consisting of a collection of tree-structured classifiers {h(x,Θk ), k=1, ...} where the {Θk} are independent identically distributed random vectors and each tree casts a unit vote for the most popular class at input x.[@Breiman_undated-tp] This paper describes the foundation and concepts of the random forest algorithm and theorizes its ability to improve classification accuracy by combining multiple decision trees. The goal of the paper is to demonstrate the effectiveness of random forests in prediction. The paper uses Amit and Geman [1997] analysis to explain that the accuracy of random forests depends on the strength of the individual tree classifiers and a measure of the dependence between them. It states the various formulas of the algorithm that are responsible for characterizing accuracy of random forests. Additionally, the author discusses using random features for lowering generalization errors than other algorithms including the use of out-of-bag estimates to monitor error, strength and correlation. He discusses his experiments and results and the use of the algorithm with bagging and without. Furthermore, he discusses all the advantages of random forest method: handling large datasets with high dimentionality, providing estimates of variable importance, and dealing with missing data. He states that the use of random inputs and random features while using Random Forest methodology produce good results in classification, less in regression examples. Additionally, he observed lower error rates on his tests on larger data sets as opposed to smaller datasets and suggested that different injections of randomness can produce better results.

## Lit Review 4 {.smaller .scrollable}

The paper ‘Random Forests for Classification in Ecology’[@Cutler2007-pe] is an interesting one since it discusses the use of Random Forests method by ecologists where this method hasn’t been broadly used in this field yet. The paper sets out to demonstrate it’s many uses in Ecology through the examples it discusses.  The Random Forests (RF) algorithm operates by fitting numerous classification trees to a data set and then combines their predictions. It begins by generating many bootstrap samples from the original data, with each sample containing about 63% of the data selected with replacement. The remaining data, which do not appear in these samples, are known as out-of-bag observations. For each bootstrap sample, a classification tree is constructed, but at each decision point within the tree, only a random subset of variables is considered for splitting the data. This randomness in variable selection helps to create diverse trees. Once all the trees are fully grown, they are used to predict the classes of the out-of-bag observations. The final prediction for each observation is made by taking a majority vote across all the trees, with ties being resolved randomly. This ensemble approach enhances the accuracy and stability of the predictions compared to individual trees. 
Random forests method can be particularly beneficial with ecological data since i) ecological data is often high dimensional with nonlinear and complex interactions among variables and ii) has many missing values among measured variables. Traditional statistical methods such as GLMs may lack in uncovering patterns and relationships we are seeking.  Three different ecological data sets were used in this study: invasive plant species, rare lichen species, and cavity-nesting birds. Using these data sets, the Random forests method was compared to four other classification methods – LDA, logistic regression, additive logistic regression, and classification trees.  Following predictions were made: the presence of invasive species in Lava Beds National Monument, presence of rare lichen species in the Pacific Northwest, and nest presence for cavity-nesting birds in Uinta Mountains, Utah. The random forest method showed high accuracy across all three data sets especially in identifying presences and identifying absences. The paper finally encourages the random forest method’s broader adoption in the field of ecology.[@Cutler2007-pe]

## Lit Review 5 {.smaller .scrollable}

The paper written by J.R. Quinlan, Simplifying Decision Trees [@Quinlan1987-cz] discusses decision tree usage within expert/artificial intelligence systems. The problem discussed is that decision trees become too complex too quickly which makes them hard to understand and use in expert systems. Four methods to simplify decision trees discussed are as follows: Cost-Complexity Pruning, Reduced Error Pruning, Pessimistic Pruning and Simplifying to Production Rules.  Each of these methods simplifies the trees by using different pruning ways and removing irrelevant conditions. In order to test these simplified methods, data from six different domains were chosen: Hypothyroid diagnosis, Discordant assay, LEDDigits, Consumer credit applications, Chess Endgame, and Probabilistic classification over disjunctions. Average size of the trees is noted on each: before applying the simplifying methods and after. All pruning methods significantly reduced the complexity of the decision trees. For example, within the Hypothyroid domain, original tree had 23.6 nodes and after applying pruning they went down to between 11.0 and 14.4.  Production rules method achieved the most sizeable simplification – reducing to just 3.0 rules which makes it highly interpretable. The same was true with the Endgame domain. Original tree had 88.8 nodes. After applying the cost complexity pruning and production rules, it went down to 51.0 nodes and 11.6 rules respectively. Through the process of testing, the study was able to prove that simplifying decision trees does lead to better representation, is easier to understand and is useful in producing knowledge for expert systems.[@Quinlan1987-cz] 


## Lit Review 6 {.smaller .scrollable}

The book ‘Data Mining with decision trees: theory and applications’[@Rokach_undated-sg] explains the concepts of decision trees in detail and also all the benefits of this methodology. Due to it’s simple technique in predicting and explaining relationships between measurements about an item and its target value, the use of Decision Trees is very popular and common in the world of data mining. There are several key features of decision trees that are advantageous. Decision Trees are self-explanatory, easy to follow, has relatively small computational effort and high predictive performance, useful for large datasets and is flexible in handling various types of data like nominal, numeric, and textual. Decision Trees classify a target as a recursive partition. It consists of nodes that form a “root” with no incoming edges.  Decision Trees are popular for their simplicity and transparency, therefore, if decision trees become too complicated, in other words have too many nodes, they become useless. For complex trees, other procedures should be developed to simplify interpretation. The book covers pre-pruning, post-pruning and cost-complexity pruning to prevent overfitting and improve generalization of decision trees. The book also examines cross-validation and bootstrapping as validation metrics for evaluating performance, and accuracy of decision trees. The book touches upon some common algorithms for decision tree induction such as ID3, C4.5, CART, CHAID, and QUEST. It also details the disadvantages of decision trees. One such disadvantage is the nearsighted nature of decision tree induction algorithms where inducers look only one level ahead. Such strategies prefer tests that score high in isolation and may overlook combinations of attributes. Using deeper lookahead strategies is considered computationally expensive and not proven useful. All in all, the book is a great guide for anyone that wants to understand decision trees in data mining and its application to practical use cases. [@Rokach_undated-sg]

## Literature Review {.smaller}

-   RF outperforms models like Naive Bayes and SVM by adjusting tree numbers and feature selection, using AUC as the main performance metric.[@Khoshgoftaar2007-bo]

-   RF handles both classification and regression tasks by creating multiple decision trees, offering resilience against overfitting.[@Biau2016-ac]

-   RF outperforms logistic regression in predicting retention and profitability, emphasizing the role of sales behavior in customer outcomes.[@Lariviere2005-uq]

-   RF accurately identifies core suppliers using e-invoice data with an AUC of 89.58%, improving procurement accuracy while reducing human bias.[@Hong2018-fr]

-   RF, using Non-Negative Matrix Factorization (NMF), classifies customer reviews with 81% precision, offering valuable insights for tailoring business offerings to specific customer needs.[@Torizuka2018-if]

-   Random Credal RF uses credal sets for increased robustness and accuracy, handling noisy datasets better than Oblique RF, but with added complexity.[@Mantas2019-li]

## References

